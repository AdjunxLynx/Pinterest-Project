from pyspark.sql import SparkSession
from pyspark import SparkContext, SparkConf
import os
import boto3

os.environ["PYSPARK_SUBMIT_ARGS"] = "--packages com.amazonaws:aws-java-sdk-s3:1.12.196,org.apache.hadoop:hadoop-aws:3.3.1 pyspark-shell"

conf = SparkConf() \
    .setAppName('S3toSpark') \
    .setMaster('local[*]')

sc=SparkContext(conf=conf)



# Configure the setting to read from the S3 bucket
accessKeyId="AKIAWU5CQUVY2FKMX5BG"
secretAccessKey="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"
hadoopConf = sc._jsc.hadoopConfiguration()
hadoopConf.set('fs.s3a.access.key', accessKeyId)
hadoopConf.set('fs.s3a.secret.key', secretAccessKey)
hadoopConf.set('spark.hadoop.fs.s3a.aws.credentials.provider', 'org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider') # Allows the package to authenticate with AWS

# Create our Spark session
spark=SparkSession(sc)

session = boto3.Session(aws_access_key_id = accessKeyId, aws_secret_access_key = secretAccessKey)

s3 = session.resource("s3")
my_bucket= s3.Bucket("kamilsawsbucket")
for obj in my_bucket.objects.all():
    print(obj.key)
    print("@@@@@@@@@@@@@@@@@@@@@@@@")



# Read from the S3 bucket
df = spark.read.json("s3a://kamilsawsbucket//api_renamed") 
# You may want to change this to read csv depending on the files your reading from the bucket
df.show()